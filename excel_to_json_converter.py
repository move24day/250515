# excel_to_json_converter.py

import streamlit as st
import json
import re
from datetime import datetime, date
import pytz
import pandas as pd

# í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”©
try:
    import google_drive_helper as gdrive
    import data # data.pyì˜ METHOD_OPTIONS, DEFAULT_STORAGE_TYPE ë“±ì„ ìœ„í•¨
except ImportError as e:
    st.error(f"í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}. (google_drive_helper.py, data.py ë“±ì„ í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìœ„ì¹˜ì‹œì¼œì£¼ì„¸ìš”)")
    st.stop()

# ì‹œê°„ëŒ€ ì„¤ì •
try:
    KST = pytz.timezone("Asia/Seoul")
except pytz.UnknownTimeZoneError:
    st.warning("Asia/Seoul ì‹œê°„ëŒ€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ UTCë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‚ ì§œ ì²˜ë¦¬ì— ì˜í–¥ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    KST = pytz.utc

# --- MOVE_TYPE_OPTIONS ì •ì˜ ---
MOVE_TYPE_OPTIONS = list(data.item_definitions.keys()) if hasattr(data, 'item_definitions') and data.item_definitions and isinstance(data.item_definitions, dict) else ["ê°€ì • ì´ì‚¬ ğŸ ", "ì‚¬ë¬´ì‹¤ ì´ì‚¬ ğŸ¢"]

# ê¸°ë³¸ê°’ ì„¤ì •
DEFAULT_CUSTOMER_NAME = "ë¬´ëª…"
DEFAULT_MOVE_TYPE = MOVE_TYPE_OPTIONS[0] if MOVE_TYPE_OPTIONS else "ê°€ì • ì´ì‚¬ ğŸ "
DEFAULT_STORAGE_TYPE = data.DEFAULT_STORAGE_TYPE if hasattr(data, 'DEFAULT_STORAGE_TYPE') else "ì»¨í…Œì´ë„ˆ ë³´ê´€ ğŸ“¦"
DEFAULT_FROM_METHOD = data.METHOD_OPTIONS[0] if hasattr(data, 'METHOD_OPTIONS') and data.METHOD_OPTIONS else "ì‚¬ë‹¤ë¦¬ì°¨ ğŸªœ"
DEFAULT_TO_METHOD = data.METHOD_OPTIONS[0] if hasattr(data, 'METHOD_OPTIONS') and data.METHOD_OPTIONS else "ì‚¬ë‹¤ë¦¬ì°¨ ğŸªœ"
STAIR_METHOD_DEFAULT = "ê³„ë‹¨ ğŸš¶" # data.py ì— ì •ì˜ëœ "ê³„ë‹¨" ì‘ì—… ë°©ë²•ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
TODAY_ISO_DATE = datetime.now(KST).date().isoformat()

# --- ê³µí†µ í—¬í¼ í•¨ìˆ˜ ---
def parse_date_flexible(date_str_input, current_year):
    if isinstance(date_str_input, (datetime, date)):
        return date_str_input.strftime('%Y-%m-%d')
    if not date_str_input or str(date_str_input).strip().lower() == "ë¯¸ì •":
        return TODAY_ISO_DATE
    date_str = str(date_str_input).strip()
    # ì‹œê°„ ì •ë³´ ì œê±° (ì˜ˆ: "06ì›” 30ì¼ 10ì‹œ" -> "06ì›” 30ì¼")
    date_str = re.split(r'\s+[0-9]{1,2}\s*[:ì‹œ]', date_str)[0].strip()
    patterns = [
        # YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD, YYYYë…„ MMì›” DDì¼
        (r'(\d{4})\s*[-/ë…„\.]?\s*(\d{1,2})\s*[-/ì›”\.]?\s*(\d{1,2})\s*(ì¼?)', lambda m: (int(m.group(1)), int(m.group(2)), int(m.group(3)))),
        # MMì›” DDì¼ (ì˜¬í•´ ê¸°ì¤€)
        (r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼?', lambda m: (current_year, int(m.group(1)), int(m.group(2)))),
        # MM/DD, MM-DD, MM.DD (ì˜¬í•´ ê¸°ì¤€)
        (r'(\d{1,2})\s*[-/.]\s*(\d{1,2})\s*', lambda m: (current_year, int(m.group(1)), int(m.group(2)))),
        # YY-MM-DD, YY/MM/DD, YY.MM.DD (20YYë…„ ê¸°ì¤€)
        (r'(\d{2})\s*[-/ë…„\.]?\s*(\d{1,2})\s*[-/ì›”\.]?\s*(\d{1,2})\s*(ì¼?)', lambda m: (2000 + int(m.group(1)), int(m.group(2)), int(m.group(3))))
    ]
    for pattern, extractor in patterns:
        match = re.match(pattern, date_str)
        if match:
            try:
                # ì „ì²´ ë¬¸ìì—´ì´ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ì˜ˆ: "ì´ìˆœì„06ì›”30ì¼" ë°©ì§€)
                # ì •ê·œí™”ëœ ë§¤ì¹˜ì™€ ì •ê·œí™”ëœ ì…ë ¥ ë¬¸ìì—´ ë¹„êµ
                normalized_match = "".join(match.group(0).split())
                normalized_date_str = "".join(date_str.split()) # ì…ë ¥ë„ ì •ê·œí™”
                if normalized_match == normalized_date_str:
                    year, month, day = extractor(match)
                    return datetime(year, month, day).strftime('%Y-%m-%d')
            except ValueError:
                continue
    return TODAY_ISO_DATE # ì–´ë–¤ íŒ¨í„´ê³¼ë„ ë§ì§€ ì•Šìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ

def normalize_phone_number_for_filename(phone_str):
    if not phone_str or not isinstance(phone_str, str): return None
    return "".join(filter(str.isdigit, phone_str))

def get_default_state(): # ëª¨ë“  í•„ìš”í•œ í‚¤ì— ëŒ€í•œ ê¸°ë³¸ê°’ ì„¤ì •
    return {
        "moving_date": TODAY_ISO_DATE, "customer_name": DEFAULT_CUSTOMER_NAME, "customer_phone": "",
        "base_move_type": DEFAULT_MOVE_TYPE, "from_location": "", "to_location": "", "special_notes": "",
        "from_floor": "", "to_floor": "",
        "from_method": DEFAULT_FROM_METHOD, "to_method": DEFAULT_TO_METHOD,
        "is_storage_move": False, "storage_type": DEFAULT_STORAGE_TYPE,
        "apply_long_distance": False, "has_via_point": False,
        "deposit_amount": 0, "adjustment_amount": 0,
        "issue_tax_invoice": False, "card_payment": False, "remove_base_housewife": False, "remove_base_man": False, # remove_base_man ì¶”ê°€
        "dispatched_1t":0, "dispatched_2_5t":0, "dispatched_3_5t":0, "dispatched_5t":0,
        "sky_hours_from": 1, "sky_hours_final": 1,
        "waste_tons_input": 0.5, "has_waste_check": False,
        "uploaded_image_paths": [],
        "vehicle_select_radio": "ìë™ ì¶”ì²œ ì°¨ëŸ‰ ì‚¬ìš©", "manual_vehicle_select_value": None,
        "final_selected_vehicle": None, "recommended_vehicle_auto": None,
        "storage_duration": 1, "storage_use_electricity": False,
        "long_distance_selector": data.long_distance_options[0] if hasattr(data, 'long_distance_options') and data.long_distance_options else "ì„ íƒ ì•ˆ í•¨",
        "via_point_location": "", "via_point_method": DEFAULT_FROM_METHOD, # via_point_methodë„ ê¸°ë³¸ê°’ ì„¤ì •
        "via_point_floor": "", "via_point_surcharge": 0,
        "departure_ladder_surcharge_manual": 0, "arrival_ladder_surcharge_manual": 0, # ìˆ˜ë™ ì‚¬ë‹¤ë¦¬ì°¨ ë¹„ìš© í‚¤ ì¶”ê°€
        "manual_ladder_from_check": False, "manual_ladder_to_check": False, # ìˆ˜ë™ ì‚¬ë‹¤ë¦¬ì°¨ ì²´í¬ë°•ìŠ¤ í‚¤ ì¶”ê°€
        "date_opt_0_widget": False, "date_opt_1_widget": False, "date_opt_2_widget": False,
        "date_opt_3_widget": False, "date_opt_4_widget": False,
        "total_volume": 0.0, "total_weight": 0.0,
        "tab3_deposit_amount": 0, "tab3_adjustment_amount": 0, "tab3_departure_ladder_surcharge_manual": 0, "tab3_arrival_ladder_surcharge_manual":0, # tab3 ê´€ë ¨ í‚¤ë“¤ ì¶”ê°€
        "tab3_date_opt_0_widget": False, "tab3_date_opt_1_widget": False, "tab3_date_opt_2_widget": False,
        "tab3_date_opt_3_widget": False, "tab3_date_opt_4_widget": False,
        "prev_final_selected_vehicle": None,
        "contract_date": TODAY_ISO_DATE, # ê³„ì•½ì¼
        "move_time_option": "ì˜¤ì „", "afternoon_move_details": "", # ì´ì‚¬ ì‹œê°„ ê´€ë ¨
        "from_address_full": "", "to_address_full": "" # ui_tab1.py ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ì†Œ í‚¤
    }


def extract_floor_from_address_enhanced(address_str):
    if not address_str or not isinstance(address_str, str):
        return address_str if address_str else "", "" # ì…ë ¥ì´ ì—†ê±°ë‚˜ ë¬¸ìì—´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜, ì£¼ì†Œë¶€ë¶„ì€ ì›ë³¸, ì¸µìˆ˜ëŠ” ë¹ˆ ë¬¸ìì—´
    address_cleaned = address_str.strip()
    parsed_floor = ""
    address_part = address_cleaned # ê¸°ë³¸ì ìœ¼ë¡œ ì£¼ì†Œ ë¶€ë¶„ì€ ì „ì²´ í´ë¦°ëœ ì£¼ì†Œ

    # 1. "...í˜¸" íŒ¨í„´ìœ¼ë¡œ ì¸µìˆ˜ ì¶”ì¶œ ì‹œë„ (ê°€ì¥ êµ¬ì²´ì ì¸ íŒ¨í„´ ìš°ì„ )
    #    "101ë™ 1203í˜¸" -> 12ì¸µ, "302í˜¸" -> 3ì¸µ
    ho_match = re.search(r'(\d+)\s*í˜¸(?!\d)', address_cleaned) # "í˜¸" ë’¤ì— ìˆ«ìê°€ ë°”ë¡œ ì˜¤ì§€ ì•ŠëŠ” ê²½ìš°
    if ho_match:
        ho_number_str = ho_match.group(1)
        if len(ho_number_str) > 2: # ì˜ˆ: 1203í˜¸ -> 12ì¸µ
            parsed_floor = ho_number_str[:-2]
        elif len(ho_number_str) > 0: # ì˜ˆ: 302í˜¸ -> 3ì¸µ, 22í˜¸ -> ì¸µìˆ˜ ì•„ë‹˜ (ì •ì±…ì— ë”°ë¼ 2ì¸µìœ¼ë¡œ ë³¼ ìˆ˜ë„ ìˆìŒ)
             # 1-2ìë¦¬ "í˜¸" ì• ìˆ«ìëŠ” ì¸µìˆ˜ë¡œ ê°„ì£¼í•˜ì§€ ì•Šê±°ë‚˜, ë§¤ìš° ë‚®ì€ ì¸µ(ì˜ˆ: 1ì¸µ, 2ì¸µ)ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ.
             # ì—¬ê¸°ì„œëŠ” 1-2ìë¦¬ëŠ” ì¸µìˆ˜ë¡œ ì•ˆ ë´„. ë§Œì•½ ë´ì•¼ í•œë‹¤ë©´ parsed_floor = ho_number_str ë¡œ ë³€ê²½.
             # í˜„ì¬ ìš”ì²­ì— ë”°ë¼, 3ìë¦¬ ì´ìƒì¼ ë•Œë§Œ ì• ë¶€ë¶„ì„ ì¸µìœ¼ë¡œ ì¸ì‹.
             # ì‚¬ìš©ì ìš”ì²­: "...í˜¸ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ê²ƒì€ ë„ì°©ì§€ ì£¼ì†Œì•¼" -> ì´ í•¨ìˆ˜ëŠ” ì¸µìˆ˜ë§Œ ì¶”ì¶œ.
             # "í˜¸" ê¹Œì§€ê°€ ì£¼ì†Œì˜ ì¼ë¶€ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì£¼ì†Œ ë¶€ë¶„ì—ì„œ "í˜¸"ë¥¼ ìë¥´ì§€ ì•ŠìŒ.
             # ì´ í•¨ìˆ˜ëŠ” ì£¼ë¡œ ì¸µìˆ˜ "ìˆ«ì"ë¥¼ ë¹¼ë‚´ëŠ” ë° ì§‘ì¤‘.
             pass # 1-2ìë¦¬ëŠ” ì¸µìˆ˜ë¡œ ì•ˆë´„. í•„ìš”ì‹œ ìˆ˜ì •.

        # "í˜¸" ì •ë³´ê°€ ì¸µìˆ˜ ì¶”ì¶œì— ì‚¬ìš©ë˜ì—ˆë‹¤ë©´, ì£¼ì†Œ ë¶€ë¶„ì—ì„œ í•´ë‹¹ "í˜¸" ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜,
        # ëª…ì‹œì ìœ¼ë¡œ ì œê±°í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë‚˜, í˜„ì¬ëŠ” "í˜¸"ê¹Œì§€ ì£¼ì†Œë¡œ í¬í•¨.
        # address_part = address_cleaned[:ho_match.start(0)].strip() # "í˜¸" ì•ë¶€ë¶„ê¹Œì§€ë§Œ ì£¼ì†Œë¡œ.
        if parsed_floor: # "í˜¸"ì—ì„œ ìœ íš¨í•œ ì¸µìˆ˜ê°€ ë‚˜ì™”ìœ¼ë©´ ë°˜í™˜ (ì£¼ì†ŒëŠ” ì „ì²´ ìœ ì§€)
            return address_cleaned, parsed_floor


    # 2. "Nì¸µ", "N F", "N f" íŒ¨í„´ìœ¼ë¡œ ì¸µìˆ˜ ì¶”ì¶œ (ì£¼ì†Œì˜ ëì— ì˜¤ëŠ” ê²½ìš°)
    #    ì˜ˆ: "ê°•ë‚¨ë¹Œë”© 5ì¸µ", "ì—˜ì§€ì•„íŒŒíŠ¸ 10 F"
    #    ì£¼ì†Œ ì¤‘ê°„ì— "5ì¸µìƒê°€" ê°™ì€ ê²½ìš°ëŠ” ì—¬ê¸°ì„œ ì²˜ë¦¬ ì•ˆ í•¨.
    floor_ending_match = re.search(r'^(.*?)(\s*(-?\d+)\s*(ì¸µ|F|f))$', address_cleaned, re.IGNORECASE)
    if floor_ending_match:
        address_part = floor_ending_match.group(1).strip() # "ì¸µ" ì•ë¶€ë¶„
        parsed_floor = floor_ending_match.group(3)     # ìˆ«ì ë¶€ë¶„
        return address_part, parsed_floor

    # 3. "ì§€í•˜ Nì¸µ", "B Nì¸µ", "B N F" íŒ¨í„´ (ì£¼ì†Œì˜ ëì— ì˜¤ëŠ” ê²½ìš°)
    #    ì˜ˆ: "ìƒê°€ B2ì¸µ", "ê±´ë¬¼ ì§€í•˜1ì¸µ"
    basement_floor_match = re.search(r'^(.*?)\s*(ì§€í•˜|-?B|b)\s*(\d+)\s*(ì¸µ|F|f)?$', address_cleaned, re.IGNORECASE)
    if basement_floor_match:
        address_part = basement_floor_match.group(1).strip()
        parsed_floor = "-" + basement_floor_match.group(3) # ì§€í•˜ì¸µì€ ìŒìˆ˜ë¡œ
        return address_part, parsed_floor

    return address_cleaned, "" # ì–´ë–¤ íŒ¨í„´ì—ë„ ì•ˆ ë§ìœ¼ë©´ ì›ë˜ ì£¼ì†Œì™€ ë¹ˆ ì¸µìˆ˜ ë°˜í™˜


# --- í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜ (ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ ë°˜ì˜) ---
PHONE_REGEX_TEXT = re.compile(r'(01[016789]-?\d{3,4}-?\d{4}|0\d{1,2}-?\d{3,4}-?\d{4})')
MOVE_TYPE_KEYWORDS_TEXT = {"ê°€ì •": ["ê°€ì •", "ê°€"], "ì‚¬ë¬´ì‹¤": ["ì‚¬ë¬´ì‹¤", "ì‚¬"]}

def parse_line_to_json_flexible(line_text, current_year, line_number_display=""):
    state = get_default_state()
    original_line = line_text.strip()
    if not original_line: return None, None, f"{line_number_display}ë¹ˆ ì¤„"

    # 1. ì „í™”ë²ˆí˜¸ íŒŒì‹± (í•„ìˆ˜)
    phone_match = PHONE_REGEX_TEXT.search(original_line)
    if not phone_match: return None, None, f"{line_number_display}ì „í™”ë²ˆí˜¸ ì—†ìŒ (í•„ìˆ˜)"
    state["customer_phone"] = phone_match.group(0).strip()
    filename_phone_part = normalize_phone_number_for_filename(state["customer_phone"])
    if not filename_phone_part: return None, None, f"{line_number_display}ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸"

    text_before_phone = original_line[:phone_match.start()].strip()
    text_after_phone = original_line[phone_match.end():].strip()

    # 2. ë‚ ì§œ ë° ê³ ê°ëª… íŒŒì‹± (ì „í™”ë²ˆí˜¸ ì•ë¶€ë¶„)
    parts_bf_phone = [p.strip() for p in text_before_phone.split() if p.strip()]
    date_parsed_from_bf = False
    customer_name_parts_bf = []

    temp_date_candidate_bf = ""
    if len(parts_bf_phone) >= 1:
        # ìµœëŒ€ 2ê°œ íŒŒíŠ¸ê¹Œì§€ ë‚ ì§œ í›„ë³´ë¡œ ê³ ë ¤ (ì˜ˆ: "06ì›” 30ì¼", "6ì›”30ì¼", "6/30")
        potential_date_str_bf = parts_bf_phone[0]
        if len(parts_bf_phone) >= 2 and not re.search(r'\d', parts_bf_phone[1]): # ë‘ë²ˆì§¸ íŒŒíŠ¸ì— ìˆ«ìê°€ ì—†ìœ¼ë©´ ì²«ë²ˆì§¸ë§Œ
             pass
        elif len(parts_bf_phone) >= 2 : # ë‘ë²ˆì§¸ íŒŒíŠ¸ë„ ë‚ ì§œì¼ ê°€ëŠ¥ì„± (ì˜ˆ: "06ì›”", "30ì¼")
            # "ì›”"ì´ë‚˜ "ì¼"ë¡œ ëë‚˜ê±°ë‚˜ ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ê²½ìš° ì´ì–´ë¶™ì—¬ ì‹œë„
            if (parts_bf_phone[0].endswith('ì›”') and parts_bf_phone[1].endswith('ì¼')) or \
               (parts_bf_phone[0].replace('.', '').replace('-', '').replace('/', '').isdigit() and parts_bf_phone[1].replace('.', '').replace('-', '').replace('/', '').isdigit()):
                 potential_date_str_bf = parts_bf_phone[0] + " " + parts_bf_phone[1]


        parsed_dt_bf = parse_date_flexible(potential_date_str_bf, current_year)
        if parsed_dt_bf != TODAY_ISO_DATE or \
           (parsed_dt_bf == TODAY_ISO_DATE and potential_date_str_bf and potential_date_str_bf.lower() != "ë¯¸ì •" and "ì˜¤ëŠ˜" not in potential_date_str_bf): # "ì˜¤ëŠ˜"ë„ ì˜¤ëŠ˜ë‚ ì§œë¡œ ì²˜ë¦¬
            state["moving_date"] = parsed_dt_bf
            date_parsed_from_bf = True
            # ë‚ ì§œë¡œ ì‚¬ìš©ëœ íŒŒíŠ¸ ê°œìˆ˜ í™•ì¸
            num_parts_for_date = len(potential_date_str_bf.split())
            customer_name_parts_bf = parts_bf_phone[num_parts_for_date:]
        else: # ë‚ ì§œ íŒŒì‹± ì•ˆë˜ë©´ ì „ì²´ë¥¼ ì´ë¦„ í›„ë³´ë¡œ
            customer_name_parts_bf = parts_bf_phone

    state["customer_name"] = " ".join(customer_name_parts_bf) if customer_name_parts_bf else DEFAULT_CUSTOMER_NAME
    if not state["customer_name"].strip(): state["customer_name"] = DEFAULT_CUSTOMER_NAME


    # 3. ì´ì‚¬ êµ¬ë¶„, ì¶œë°œì§€, ë„ì°©ì§€, íŠ¹ì´ì‚¬í•­ íŒŒì‹± (ì „í™”ë²ˆí˜¸ ë’·ë¶€ë¶„)
    remaining_text = text_after_phone

    # 3a. ì´ì‚¬ êµ¬ë¶„ ("ê°€" ë˜ëŠ” "ì‚¬")
    #     ì²« ë‹¨ì–´ê°€ "ê°€" ë˜ëŠ” "ì‚¬"ì´ê³  ë°”ë¡œ ë’¤ì— ê³µë°±ì´ ì˜¤ê±°ë‚˜, ë‹¨ë…ìœ¼ë¡œ "ê°€", "ì‚¬"ì¸ ê²½ìš°
    if remaining_text.lower().startswith("ê°€ ") or remaining_text.lower() == "ê°€":
        state["base_move_type"] = next((opt for opt in MOVE_TYPE_OPTIONS if "ê°€ì •" in opt), DEFAULT_MOVE_TYPE)
        remaining_text = remaining_text[2:].lstrip() if remaining_text.lower().startswith("ê°€ ") else remaining_text[1:].lstrip()
    elif remaining_text.lower().startswith("ì‚¬ ") or remaining_text.lower() == "ì‚¬":
        state["base_move_type"] = next((opt for opt in MOVE_TYPE_OPTIONS if "ì‚¬ë¬´ì‹¤" in opt), DEFAULT_MOVE_TYPE) # ì‚¬ë¬´ì‹¤ ê¸°ë³¸ê°’ìœ¼ë¡œ ìˆ˜ì •í•„ìš”
        remaining_text = remaining_text[2:].lstrip() if remaining_text.lower().startswith("ì‚¬ ") else remaining_text[1:].lstrip()
    # ì´ì‚¬ êµ¬ë¶„ì´ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’(ê°€ì • ì´ì‚¬) ì‚¬ìš© (state ì´ˆê¸°ê°’ì— ì´ë¯¸ ì„¤ì •ë¨)

    # 3b. ì¶œë°œì§€ ì£¼ì†Œ ( "...í˜¸"ë¡œ ëë‚˜ëŠ” ë¶€ë¶„ê¹Œì§€) ë° ì¸µìˆ˜
    from_location_candidate = ""
    # ê°€ì¥ ë§ˆì§€ë§‰ "...í˜¸" íŒ¨í„´ì„ ì°¾ìŒ
    last_ho_match = None
    for match_ho in re.finditer(r'([^\s]+ë™\s*\d+í˜¸|\d+í˜¸)', remaining_text): # "XXXë™ YYYí˜¸" ë˜ëŠ” ê·¸ëƒ¥ "YYYí˜¸"
        last_ho_match = match_ho

    if last_ho_match:
        from_location_candidate = remaining_text[:last_ho_match.end()].strip()
        state["from_location"] = from_location_candidate
        _, state["from_floor"] = extract_floor_from_address_enhanced(from_location_candidate) # "í˜¸" ì •ë³´ì—ì„œ ì¸µìˆ˜ ì¶”ì¶œ
        remaining_text_after_from_loc = remaining_text[last_ho_match.end():].lstrip()
    else:
        # "...í˜¸"ë¥¼ ëª»ì°¾ìœ¼ë©´, ì²«ë²ˆì§¸ ì£¼ìš” ë¸”ë¡ì„ ì¶œë°œì§€ë¡œ ê°€ì • (íƒ­ì´ë‚˜ 2ì¹¸ ì´ìƒ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        first_block_match_text = re.match(r'([^\s\t]+(?:\s+[^\s\t]+)*)(\s{2,}|\t+|$)', remaining_text)
        if first_block_match_text:
            from_location_candidate = first_block_match_text.group(1).strip()
            # ì¶œë°œì§€ ì£¼ì†Œì™€ ì¸µìˆ˜ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ ì‹œë„
            addr_part, floor_part = extract_floor_from_address_enhanced(from_location_candidate)
            state["from_location"] = addr_part
            state["from_floor"] = floor_part
            remaining_text_after_from_loc = remaining_text[first_block_match_text.end():].lstrip()
        else: # ì´ê²ƒë„ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ë¥¼ ì¶œë°œì§€ë¡œ (ì¸µìˆ˜ ì—†ìŒ)
            addr_part, floor_part = extract_floor_from_address_enhanced(remaining_text) # ì „ì²´ì—ì„œ ì‹œë„
            state["from_location"] = addr_part
            state["from_floor"] = floor_part
            remaining_text_after_from_loc = "" # ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ì¶œë°œì§€ë¡œ ì‚¬ìš©ë¨

    if not state["from_location"]: # ì¶œë°œì§€ê°€ íŒŒì‹± ì•ˆëìœ¼ë©´ ì˜¤ë¥˜
        return None, None, f"{line_number_display}ì¶œë°œì§€ ì£¼ì†Œ íŠ¹ì • ë¶ˆê°€: '{original_line}'"


    # 3c. ë„ì°©ì§€ ì£¼ì†Œ ë° íŠ¹ì´ì‚¬í•­ (ì¶œë°œì§€ íŒŒì‹± í›„ ë‚¨ì€ í…ìŠ¤íŠ¸ì—ì„œ)
    #     remaining_text_after_from_loc ì—ì„œ íŒŒì‹±
    #     "...í˜¸ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ê²ƒì€ ë„ì°©ì§€ ì£¼ì†Œ" ë¼ëŠ” ìš”êµ¬ì‚¬í•­ ë°˜ì˜
    
    # ì‹œê°„ íŒ¨í„´ ì •ì˜ (ìš”ì¼/ì‹œê°„ ì •ë³´ ì œê±°ìš©)
    time_pattern = re.compile(r"([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼](ìš”ì¼)?\s*\d{1,2}\s*[:ì‹œ][^-\s]*(\s*-\s*\d{1,2}\s*[:ì‹œ][^-\s]*)?|\d{1,2}\s*[:ì‹œ][^-\s]*(\s*-\s*\d{1,2}\s*[:ì‹œ][^-\s]*)?)\s*$")

    to_location_candidate_notes_combined = remaining_text_after_from_loc # ì´ˆê¸°ê°’
    
    # 1. ì‹œê°„ ì •ë³´ê°€ ìˆë‹¤ë©´ ë¶„ë¦¬ í›„ ì œê±°
    time_match_end = time_pattern.search(to_location_candidate_notes_combined)
    if time_match_end:
        to_location_candidate_notes_combined = to_location_candidate_notes_combined[:time_match_end.start()].strip()
        # state["special_notes"] = time_match_end.group(0).strip() # ì‹œê°„ ì •ë³´ë¥¼ íŠ¹ì´ì‚¬í•­ì— ë„£ì„ ìˆ˜ë„ ìˆìŒ (ì„ íƒ)

    # 2. ë‚¨ì€ ë¶€ë¶„ì—ì„œ ë„ì°©ì§€ ì£¼ì†Œì™€ íŠ¹ì´ì‚¬í•­ ë¶„ë¦¬
    #    ì—¬ê¸°ì„œëŠ” ì²«ë²ˆì§¸ "ì£¼ìš” ë¸”ë¡" (íƒ­ ë˜ëŠ” 2ê°œì´ìƒ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ)ì„ ë„ì°©ì§€ë¡œ ê°„ì£¼
    #    ë‚˜ë¨¸ì§€ë¥¼ íŠ¹ì´ì‚¬í•­ìœ¼ë¡œ. ë§Œì•½ êµ¬ë¶„ì ì—†ìœ¼ë©´ ì „ì²´ê°€ ë„ì°©ì§€.
    if to_location_candidate_notes_combined:
        parts_for_to_loc_notes = [p.strip() for p in re.split(r'\s{2,}|\t+', to_location_candidate_notes_combined) if p.strip()]
        if parts_for_to_loc_notes:
            # ì²«ë²ˆì§¸ ë¸”ë¡ì„ ë„ì°©ì§€ë¡œ
            to_address_candidate = parts_for_to_loc_notes.pop(0)
            addr_part_to, floor_part_to = extract_floor_from_address_enhanced(to_address_candidate)
            state["to_location"] = addr_part_to
            state["to_floor"] = floor_part_to
            
            # ë‚¨ì€ ë¸”ë¡ë“¤ì´ ìˆë‹¤ë©´ íŠ¹ì´ì‚¬í•­ìœ¼ë¡œ í•©ì¹¨
            if parts_for_to_loc_notes:
                existing_notes = state.get("special_notes", "")
                new_notes = " ".join(parts_for_to_loc_notes)
                state["special_notes"] = f"{existing_notes} {new_notes}".strip() if existing_notes else new_notes
        else: # ì£¼ìš” ë¸”ë¡ êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´, ì „ì²´ë¥¼ ë„ì°©ì§€ë¡œ (íŠ¹ì´ì‚¬í•­ ì—†ìŒ)
            addr_part_to, floor_part_to = extract_floor_from_address_enhanced(to_location_candidate_notes_combined)
            state["to_location"] = addr_part_to
            state["to_floor"] = floor_part_to
            # íŠ¹ì´ì‚¬í•­ì€ ì´ ê²½ìš° ì—†ìŒ (ë˜ëŠ” ìœ„ì—ì„œ ì‹œê°„ ì •ë³´ë§Œ ë“¤ì–´ê°”ì„ ìˆ˜ ìˆìŒ)

    # ì‘ì—… ë°©ë²• ê¸°ë³¸ê°’ ì„¤ì • (ê³„ë‹¨)
    if hasattr(data, 'METHOD_OPTIONS') and STAIR_METHOD_DEFAULT in data.METHOD_OPTIONS:
        state["from_method"] = STAIR_METHOD_DEFAULT
        state["to_method"] = STAIR_METHOD_DEFAULT
    else: # data.pyì— "ê³„ë‹¨"ì´ ì—†ì„ ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬ (íŠ¹ì´ì‚¬í•­ì— ëª…ì‹œ)
        current_notes = state.get("special_notes", "")
        stair_method_name_for_note = STAIR_METHOD_DEFAULT.split(" ")[0] # "ê³„ë‹¨"
        note_addition = f"(ì°¸ê³ : ìš”ì²­ëœ '{stair_method_name_for_note}' ì‘ì—…ë°©ë²•ì„ data.pyì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©)"
        state["special_notes"] = f"{current_notes} {note_addition}".strip() if current_notes else note_addition


    # ìµœì¢… í•„ìˆ˜ í•„ë“œ (ì¶œë°œì§€) í™•ì¸
    if not state.get("from_location"):
        return None, None, f"{line_number_display}ì¶œë°œì§€ ì£¼ì†Œ ìµœì¢… íŒŒì‹± ì‹¤íŒ¨."

    return state, filename_phone_part + ".json", None


# --- ì—‘ì…€ ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜ ---
COLUMN_ALIASES_EXCEL = {
    'moving_date': ['ë‚ ì§œ', 'ì´ì‚¬ë‚ ì§œ', 'ì¼ì'],
    'customer_name': ['ê³ ê°ëª…', 'ì´ë¦„', 'ì„±í•¨', 'ìƒí˜¸'],
    'customer_phone': ['ì „í™”ë²ˆí˜¸', 'ì—°ë½ì²˜', 'íœ´ëŒ€í°ë²ˆí˜¸', 'ì „í™”', 'í•¸ë“œí°', 'H.P', 'HP'],
    'base_move_type': ['ì´ì‚¬ì¢…ë¥˜', 'êµ¬ë¶„', 'ì¢…ë¥˜'],
    'from_location': ['ì¶œë°œì§€ì£¼ì†Œ', 'ì¶œë°œì§€', 'ì¶œë°œì£¼ì†Œ', 'ì¶œë°œ'],
    'from_floor': ['ì¸µìˆ˜', 'ì¶œë°œì§€ ì¸µìˆ˜', 'ì¶œë°œì¸µìˆ˜', 'ì¶œë°œ ì¸µ'],
    'to_location': ['ë„ì°©ì§€ì£¼ì†Œ', 'ë„ì°©ì§€', 'ë„ì°©ì£¼ì†Œ', 'ë„ì°©'],
    'to_floor': ['ë„ì°©ì§€ ì¸µìˆ˜', 'ë„ì°©ì¸µìˆ˜', 'ë„ì°© ì¸µ'],
    'special_notes': ['íŠ¹ì´ì‚¬í•­', 'ìš”êµ¬ì‚¬í•­', 'í¬ë§ì‚¬í•­', 'ê±´ì˜', 'ë©”ëª¨', 'ë¹„ê³ ', 'ì°¸ê³ ì‚¬í•­'],
}
def get_column_value(row, field_name, aliases, default=""):
    # ì»¬ëŸ¼ëª… ë¹„êµ ì‹œ ì†Œë¬¸ìë¡œ í†µì¼, ê³µë°± ì œê±°
    row_index_lower_stripped = {str(idx_item).lower().replace(" ", ""): idx_item for idx_item in row.index}
    
    # ê¸°ë³¸ í•„ë“œëª… (ì†Œë¬¸ì, ê³µë°±ì œê±°)
    standard_field_lower_stripped = field_name.lower().replace(" ", "")
    if standard_field_lower_stripped in row_index_lower_stripped and pd.notna(row[row_index_lower_stripped[standard_field_lower_stripped]]):
        return str(row[row_index_lower_stripped[standard_field_lower_stripped]]).strip()

    # ë³„ì¹­ í™•ì¸ (ì†Œë¬¸ì, ê³µë°±ì œê±°)
    for alias_item in aliases.get(field_name, []):
        alias_lower_stripped = alias_item.lower().replace(" ", "")
        if alias_lower_stripped in row_index_lower_stripped and pd.notna(row[row_index_lower_stripped[alias_lower_stripped]]):
            return str(row[row_index_lower_stripped[alias_lower_stripped]]).strip()
    return default


def parse_excel_row_to_json(row, current_year, row_number_display=""):
    state = get_default_state() # ê¸°ë³¸ ìƒíƒœê°’ìœ¼ë¡œ ì‹œì‘
    # í–‰ ì „ì²´ê°€ ë¹„ì—ˆëŠ”ì§€ í™•ì¸ (ëª¨ë“  ê°’ì´ NAê±°ë‚˜ ë¹ˆ ë¬¸ìì—´)
    if row.isnull().all() or all(str(x).strip() == "" for x in row if pd.notna(x)):
        return None, None, f"{row_number_display}ë¹ˆ í–‰"

    # í•„ìˆ˜: ì „í™”ë²ˆí˜¸, ì¶œë°œì§€ ì£¼ì†Œ
    customer_phone_raw = get_column_value(row, 'customer_phone', COLUMN_ALIASES_EXCEL)
    if not customer_phone_raw: return None, None, f"{row_number_display}ì „í™”ë²ˆí˜¸ ì—†ìŒ (í•„ìˆ˜)"
    state["customer_phone"] = customer_phone_raw
    filename_phone_part = normalize_phone_number_for_filename(customer_phone_raw)
    if not filename_phone_part: return None, None, f"{row_number_display}ìœ íš¨í•˜ì§€ ì•Šì€ ì „í™”ë²ˆí˜¸"

    from_location_raw = get_column_value(row, 'from_location', COLUMN_ALIASES_EXCEL)
    if not from_location_raw: return None, None, f"{row_number_display}ì¶œë°œì§€ ì£¼ì†Œ ì—†ìŒ (í•„ìˆ˜)"
    
    # ë‚ ì§œ íŒŒì‹± ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    moving_date_raw = get_column_value(row, 'moving_date', COLUMN_ALIASES_EXCEL)
    state["moving_date"] = parse_date_flexible(moving_date_raw, current_year)
    log_info_for_date = "" # ë‚ ì§œ ì²˜ë¦¬ ê´€ë ¨ ë¡œê·¸ ë©”ì‹œì§€
    if moving_date_raw and moving_date_raw.strip().lower() != "ë¯¸ì •" and state["moving_date"] == TODAY_ISO_DATE:
        # parse_date_flexibleê°€ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ë°˜í™˜í–ˆì§€ë§Œ, ì…ë ¥ê°’ì´ "ë¯¸ì •"ì´ë‚˜ ë¹ˆ ê°’ì´ ì•„ë‹ˆì—ˆë˜ ê²½ìš°
        log_info_for_date = f"ì œê³µëœ ë‚ ì§œ '{moving_date_raw}'ê°€ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì²˜ë¦¬ë¨ (í˜•ì‹/ë‚´ìš© í™•ì¸ í•„ìš”)."

    # ê³ ê°ëª… íŒŒì‹± ë° ê¸°ë³¸ê°’, ë³´ê´€ì´ì‚¬ ì²´í¬
    customer_name_raw = get_column_value(row, 'customer_name', COLUMN_ALIASES_EXCEL)
    if customer_name_raw and customer_name_raw.lower() != "ë¯¸ì •": state["customer_name"] = customer_name_raw
    else: state["customer_name"] = DEFAULT_CUSTOMER_NAME # ë¯¸ì •ì´ê±°ë‚˜ ë¹„ì—ˆìœ¼ë©´ ê¸°ë³¸ê°’ "ë¬´ëª…"
    if "ë³´ê´€" in state["customer_name"]: # ê³ ê°ëª…ì— "ë³´ê´€" í¬í•¨ ì‹œ
        state["is_storage_move"] = True
        state["storage_type"] = DEFAULT_STORAGE_TYPE # ê¸°ë³¸ ë³´ê´€ ìœ í˜• ì„¤ì •

    # ì´ì‚¬ ì¢…ë¥˜
    move_type_raw = get_column_value(row, 'base_move_type', COLUMN_ALIASES_EXCEL)
    if move_type_raw:
        move_type_char = move_type_raw.strip().lower()
        if any(keyword == move_type_char for keyword in MOVE_TYPE_KEYWORDS_TEXT["ê°€ì •"]) or "ê°€ì •" in move_type_char :
            state["base_move_type"] = next((opt for opt in MOVE_TYPE_OPTIONS if "ê°€ì •" in opt), DEFAULT_MOVE_TYPE)
        elif any(keyword == move_type_char for keyword in MOVE_TYPE_KEYWORDS_TEXT["ì‚¬ë¬´ì‹¤"]) or "ì‚¬ë¬´ì‹¤" in move_type_char:
            state["base_move_type"] = next((opt for opt in MOVE_TYPE_OPTIONS if "ì‚¬ë¬´ì‹¤" in opt), MOVE_TYPE_OPTIONS[1] if len(MOVE_TYPE_OPTIONS)>1 else DEFAULT_MOVE_TYPE)
    # ëª…ì‹œ ì•ˆë˜ë©´ ê¸°ë³¸ê°’ (ê°€ì • ì´ì‚¬) ìœ ì§€

    # ì¶œë°œì§€ ì£¼ì†Œ ë° ì¸µìˆ˜
    from_floor_raw_col = get_column_value(row, 'from_floor', COLUMN_ALIASES_EXCEL) # ëª…ì‹œì  ì¸µìˆ˜ ì»¬ëŸ¼
    addr_part_from, floor_part_from = extract_floor_from_address_enhanced(from_location_raw)
    state["from_location"] = addr_part_from # extract_floor_from_address_enhancedê°€ ì£¼ì†Œ ë¶€ë¶„ë§Œ ë°˜í™˜
    state["from_floor"] = from_floor_raw_col if from_floor_raw_col else floor_part_from # ëª…ì‹œì  ì¸µìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ì¶”ì¶œëœ ì¸µìˆ˜

    # ë„ì°©ì§€ ì£¼ì†Œ ë° ì¸µìˆ˜
    to_location_raw = get_column_value(row, 'to_location', COLUMN_ALIASES_EXCEL)
    to_floor_raw_col = get_column_value(row, 'to_floor', COLUMN_ALIASES_EXCEL)
    if to_location_raw or to_floor_raw_col : # ë„ì°©ì§€ ì •ë³´ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´
        if to_location_raw:
            addr_part_to, floor_part_to = extract_floor_from_address_enhanced(to_location_raw)
            state["to_location"] = addr_part_to
            state["to_floor"] = to_floor_raw_col if to_floor_raw_col else floor_part_to
        else: # ë„ì°©ì§€ ì£¼ì†ŒëŠ” ì—†ê³  ì¸µìˆ˜ë§Œ ìˆëŠ” ê²½ìš°
            state["to_location"] = "" # ì£¼ì†ŒëŠ” ë¹„ì›Œë‘ 
            state["to_floor"] = to_floor_raw_col

    # íŠ¹ì´ì‚¬í•­ (ë‚ ì§œ ì²˜ë¦¬ ë¡œê·¸ì™€ í•©ì¹¨)
    special_notes_raw = get_column_value(row, 'special_notes', COLUMN_ALIASES_EXCEL)
    if log_info_for_date and special_notes_raw:
        state["special_notes"] = log_info_for_date + " " + special_notes_raw
    elif log_info_for_date:
        state["special_notes"] = log_info_for_date
    elif special_notes_raw:
        state["special_notes"] = special_notes_raw
    
    # ì—‘ì…€ ì…ë ¥ì˜ ê²½ìš° ì‘ì—…ë°©ë²•ì€ ê¸°ë³¸ê°’ ìœ ì§€ (í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ë‹¬ë¦¬ ëª…ì‹œì  ê·œì¹™ ì—†ìŒ)
    # state["from_method"] = STAIR_METHOD_DEFAULT (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # state["to_method"] = STAIR_METHOD_DEFAULT (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)

    return state, filename_phone_part + ".json", None


# --- Streamlit UI ---
st.set_page_config(page_title="ì´ì‚¬ì •ë³´ JSON ë³€í™˜ê¸°", layout="wide")
st.title("ğŸšš ì´ì‚¬ ì •ë³´ JSON ë³€í™˜ ë° Drive ì €ì¥")
st.caption("í…ìŠ¤íŠ¸ ë˜ëŠ” Excel íŒŒì¼ë¡œ ëœ ì´ì‚¬ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ JSON íŒŒì¼ë¡œ ë³€í™˜í•˜ê³  Google Driveì— ì €ì¥í•©ë‹ˆë‹¤.")

input_method = st.radio("ì…ë ¥ ë°©ì‹ ì„ íƒ:", ('í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥', 'Excel íŒŒì¼ ì—…ë¡œë“œ'), horizontal=True)
text_input = ""
uploaded_file = None

if input_method == 'í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥':
    text_input = st.text_area("ì—¬ê¸°ì— ì´ì‚¬ ì •ë³´ë¥¼ í•œ ì¤„ì”© ì…ë ¥í•˜ì„¸ìš”:", height=200,
                              placeholder="ì˜ˆì‹œ: 07ì›” 04ì¼ ì´ìˆœì„ 010-2701-0758 ê°€ ë™ëŒ€ë¬¸êµ¬ í•œì²œë¡œ63ê¸¸ 10 ì´ë¬¸eí¸í•œì„¸ìƒ 103ë™ 2101í˜¸  ë™ëŒ€ë¬¸êµ¬ íœ˜ê²½1ë™  ì´ì–‘ì‹ ìˆ˜9ì‹œ")
else:
    uploaded_file = st.file_uploader("ë³€í™˜í•  Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["xlsx", "xls"])
    st.markdown("""
    **Excel íŒŒì¼ í˜•ì‹ ê°€ì´ë“œ:**
    - ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”(ì»¬ëŸ¼ëª…)ì—¬ì•¼ í•©ë‹ˆë‹¤. ì»¬ëŸ¼ëª…ì€ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šìœ¼ë©°, ê³µë°±ì€ ë¬´ì‹œë©ë‹ˆë‹¤.
    - **í•„ìˆ˜ ì»¬ëŸ¼**: `ì „í™”ë²ˆí˜¸`, `ì¶œë°œì§€ì£¼ì†Œ` (ë˜ëŠ” ìœ ì‚¬ì–´: ì—°ë½ì²˜, ì¶œë°œì§€ ë“±)
    - **ì„ íƒ ì»¬ëŸ¼**: `ë‚ ì§œ` (ì¸ì‹ ê°€ëŠ¥í•œ í˜•ì‹, ë¯¸ì…ë ¥/ì¸ì‹ë¶ˆê°€ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ), `ê³ ê°ëª…` (ë¯¸ì…ë ¥ì‹œ 'ë¬´ëª…'), `ì´ì‚¬ì¢…ë¥˜`('ê°€'/'ì‚¬' ë˜ëŠ” 'ê°€ì •', 'ì‚¬ë¬´ì‹¤'), `ì¶œë°œì§€ ì¸µìˆ˜`, `ë„ì°©ì§€ì£¼ì†Œ`, `ë„ì°©ì§€ ì¸µìˆ˜`, `íŠ¹ì´ì‚¬í•­` (ë˜ëŠ” ìœ ì‚¬ì–´)
    - ê³ ê°ëª…ì— "ë³´ê´€"ì´ í¬í•¨ë˜ë©´ ë³´ê´€ì´ì‚¬ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.
    """)

if st.button("ğŸ”„ JSON ë³€í™˜ ë° Google Driveì— ì €ì¥í•˜ê¸°"):
    current_year_for_parsing = datetime.now(KST).year
    success_count = 0; error_count = 0; processed_items = 0; total_items = 0
    all_log_messages = []; items_to_process = []; is_excel_input = False

    if input_method == 'í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥':
        if not text_input.strip(): st.warning("ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            items_to_process = [line for line in text_input.strip().split('\n') if line.strip()]
            total_items = len(items_to_process)
    elif input_method == 'Excel íŒŒì¼ ì—…ë¡œë“œ':
        is_excel_input = True
        if uploaded_file is None: st.warning("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            try:
                try: df = pd.read_excel(uploaded_file, engine='openpyxl')
                except Exception: uploaded_file.seek(0); df = pd.read_excel(uploaded_file, engine='xlrd')
                # ì»¬ëŸ¼ëª…ì„ ì½ì„ ë•Œ ê³µë°± ì œê±° ë° ì†Œë¬¸ìí™” (get_column_value í•¨ìˆ˜ì™€ ì¼ê´€ì„±)
                df.columns = [str(col).strip().lower().replace(" ", "") for col in df.columns]
                items_to_process = [row for _, row in df.iterrows() if not row.isnull().all()]
                total_items = len(items_to_process)
            except Exception as e: st.error(f"Excel íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); items_to_process = []

    if not items_to_process:
        if text_input.strip() or uploaded_file: # ì…ë ¥ ì‹œë„ëŠ” ìˆì—ˆìœ¼ë‚˜ ì²˜ë¦¬í•  ì•„ì´í…œì´ ì—†ëŠ” ê²½ìš°
            st.info("ë³€í™˜í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        st.subheader("âœ¨ ì²˜ë¦¬ ê²°ê³¼")
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, item_data_row_or_line in enumerate(items_to_process):
            processed_items += 1
            status_obj, filename, error_msg = (None, None, "ì•Œ ìˆ˜ ì—†ëŠ” ì…ë ¥ í˜•ì‹ ë˜ëŠ” ì²˜ë¦¬ ì˜¤ë¥˜")
            row_display_prefix = f"ì—‘ì…€ {df.index[i]+2}í–‰" if is_excel_input and hasattr(df, 'index') and i < len(df.index) else \
                                 (f"ì—‘ì…€ {i+2}í–‰" if is_excel_input else f"í…ìŠ¤íŠ¸ {i+1}ì¤„")

            if is_excel_input:
                status_obj, filename, error_msg = parse_excel_row_to_json(item_data_row_or_line, current_year_for_parsing, row_display_prefix + ": ")
            else: # í…ìŠ¤íŠ¸ ì…ë ¥
                status_obj, filename, error_msg = parse_line_to_json_flexible(item_data_row_or_line, current_year_for_parsing, row_display_prefix + ": ")

            status_text.text(f"ì²˜ë¦¬ ì¤‘... {processed_items}/{total_items} ({filename if filename else 'ë°ì´í„° ë¶„ì„ ì¤‘'})")
            progress_bar.progress(processed_items / total_items if total_items > 0 else 0)

            log_identifier_parts = []
            if status_obj and status_obj.get('customer_phone'): log_identifier_parts.append(status_obj['customer_phone'])
            if status_obj and status_obj.get('customer_name') != DEFAULT_CUSTOMER_NAME : log_identifier_parts.append(status_obj['customer_name'])
            log_identifier = f"({', '.join(log_identifier_parts)})" if log_identifier_parts else ""

            # ì—‘ì…€ì—ì„œ ë‚ ì§œ íŒŒì‹± ê´€ë ¨ ë¡œê·¸ (parse_excel_row_to_json í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ special_notesì— ì´ë¯¸ ì¶”ê°€ë¨)
            # ì¤‘ë³µ ë¡œê·¸ ë°©ì§€ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ. í•„ìš”ì‹œ parse_excel_row_to_jsonì˜ ë¡œê·¸ ë°©ì‹ì„ ë³€ê²½.

            if status_obj and filename:
                final_state_to_save = get_default_state() # í•­ìƒ ëª¨ë“  í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ê¸°ë³¸ ìƒíƒœì—ì„œ ì‹œì‘
                final_state_to_save.update(status_obj)    # íŒŒì‹±ëœ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                try:
                    gdrive_folder_id_secret = st.secrets.get("gcp_service_account", {}).get("drive_folder_id")
                    save_result = gdrive.save_json_file(filename, final_state_to_save, folder_id=gdrive_folder_id_secret)
                    if save_result and save_result.get('id'):
                        log_message = f"âœ”ï¸ <span style='color:green;'>ì €ì¥ ì„±ê³µ</span>: {filename} {log_identifier} (ID: {save_result.get('id')})"
                        all_log_messages.append(log_message); success_count += 1
                    else:
                        log_message = f"âŒ <span style='color:red;'>ì €ì¥ ì‹¤íŒ¨</span>: {filename} {log_identifier} (ì‘ë‹µ: {save_result})"
                        all_log_messages.append(log_message); error_count += 1
                except AttributeError as ae: # gdrive ëª¨ë“ˆ ê´€ë ¨ ì˜¤ë¥˜ ë“±
                     log_message = f"âŒ <span style='color:red;'>ì €ì¥ í•¨ìˆ˜ ì˜¤ë¥˜</span>: {filename} {log_identifier} (ì˜¤ë¥˜: {ae})"
                     all_log_messages.append(log_message); error_count += 1
                except Exception as e_save:
                    log_message = f"âŒ <span style='color:red;'>ì €ì¥ ì¤‘ ì˜ˆì™¸</span>: {filename if filename else 'ë°ì´í„°'} {log_identifier} ({str(e_save)})"
                    all_log_messages.append(log_message); error_count += 1
            else:
                log_message = f"âš ï¸ <span style='color:orange;'>ê±´ë„ˆëœ€/ì˜¤ë¥˜</span>: {error_msg if error_msg else 'ì‚¬ìœ  ë¶ˆëª…'} {log_identifier}"
                all_log_messages.append(log_message); error_count +=1

        status_text.empty(); progress_bar.empty()
        st.info(f"ì´ ë¶„ì„ ëŒ€ìƒ: {total_items} ê±´ (ì‹¤ì œ ì²˜ë¦¬ ì‹œë„: {processed_items} ê±´)")
        st.success(f"Google Drive ì €ì¥ ì„±ê³µ: {success_count} ê±´")
        if error_count > 0: st.error(f"ì‹¤íŒ¨ ë˜ëŠ” ê±´ë„ˆëœ€: {error_count} ê±´")
        else: st.info(f"ì‹¤íŒ¨ ë˜ëŠ” ê±´ë„ˆëœ€: {error_count} ê±´") # ì‹¤íŒ¨ê°€ 0ê±´ì¼ ë•Œë„ ì •ë³´ì„±ìœ¼ë¡œ í‘œì‹œ

        if all_log_messages:
            # ì˜¤ë¥˜ê°€ ìˆê±°ë‚˜, ì„±ê³µ ê±´ìˆ˜ê°€ ì „ì²´ë³´ë‹¤ ì ê±°ë‚˜, "ì •ë³´" ë¡œê·¸ê°€ ìˆì„ ê²½ìš° ë¡œê·¸ ì°½ì„ ê¸°ë³¸ìœ¼ë¡œ í¼ì¹¨
            expanded_log = (error_count > 0 or success_count < total_items or any("ì •ë³´" in log for log in all_log_messages))
            with st.expander("â–¼ ìƒì„¸ ì²˜ë¦¬ ë¡œê·¸ ë³´ê¸° (í´ë¦­)", expanded=expanded_log):
                log_html = "".join([f"<div style='font-size:small; margin-bottom:3px;'>{log}</div>" for log in all_log_messages])
                st.markdown(log_html, unsafe_allow_html=True)
